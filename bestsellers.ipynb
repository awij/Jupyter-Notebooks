{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bestsellers from Amazon 2009-2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import string\n",
    "import sklearn.preprocessing as preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into a Dataframe\n",
    "For the purpose of demonstrating the KNN supervised machine-learning algorithm we will be using the top 50 Bestsellers, from 2009 to 2019, both fiction and non-fiction. In the markdown below you will find a direct download link from Kaggle. Assuming you have downloaded it into your working directory, the code below should load the data into an easily accessible DataFrame format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bestsellers_categories.csv')\n",
    "\n",
    "# Download link: https://www.kaggle.com/sootersaalu/amazon-top-50-bestselling-books-2009-2019/download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview data for brief synopsis\n",
    "In this example, we review the value counts for each column to get an idea of what the data is illustrating, and where we may go in regards to compiling a set of features for the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Publication Manual of the American Psychological Association, 6th Edition       10\n",
      "StrengthsFinder 2.0                                                              9\n",
      "Oh, the Places You'll Go!                                                        8\n",
      "The 7 Habits of Highly Effective People: Powerful Lessons in Personal Change     7\n",
      "The Very Hungry Caterpillar                                                      7\n",
      "                                                                                ..\n",
      "Astrophysics for People in a Hurry                                               1\n",
      "Howard Stern Comes Again                                                         1\n",
      "National Geographic Kids Why?: Over 1,111 Answers to Everything                  1\n",
      "Magnolia Table: A Collection of Recipes for Gathering                            1\n",
      "Guts                                                                             1\n",
      "Name: Name, Length: 351, dtype: int64\n",
      "Jeff Kinney                           12\n",
      "Gary Chapman                          11\n",
      "Suzanne Collins                       11\n",
      "Rick Riordan                          11\n",
      "American Psychological Association    10\n",
      "                                      ..\n",
      "Phil Robertson                         1\n",
      "Mike Moreno                            1\n",
      "Alan Moore                             1\n",
      "David Goggins                          1\n",
      "Mark Twain                             1\n",
      "Name: Author, Length: 248, dtype: int64\n",
      "4.8    127\n",
      "4.7    108\n",
      "4.6    105\n",
      "4.5     60\n",
      "4.9     52\n",
      "4.4     38\n",
      "4.3     25\n",
      "4.0     14\n",
      "4.2      8\n",
      "4.1      6\n",
      "3.9      3\n",
      "3.8      2\n",
      "3.3      1\n",
      "3.6      1\n",
      "Name: User Rating, dtype: int64\n",
      "8580     10\n",
      "5069      9\n",
      "21834     8\n",
      "19546     7\n",
      "19576     6\n",
      "         ..\n",
      "3428      1\n",
      "14982     1\n",
      "9867      1\n",
      "13964     1\n",
      "2052      1\n",
      "Name: Reviews, Length: 346, dtype: int64\n",
      "8      52\n",
      "6      38\n",
      "9      38\n",
      "5      36\n",
      "11     35\n",
      "4      32\n",
      "14     30\n",
      "13     29\n",
      "10     28\n",
      "12     27\n",
      "7      23\n",
      "15     21\n",
      "16     20\n",
      "17     19\n",
      "20     17\n",
      "18     14\n",
      "0      12\n",
      "46     10\n",
      "21      9\n",
      "28      6\n",
      "22      6\n",
      "40      5\n",
      "23      5\n",
      "24      5\n",
      "2       5\n",
      "30      5\n",
      "19      4\n",
      "27      4\n",
      "25      2\n",
      "32      2\n",
      "105     2\n",
      "82      1\n",
      "36      1\n",
      "39      1\n",
      "42      1\n",
      "52      1\n",
      "53      1\n",
      "3       1\n",
      "54      1\n",
      "1       1\n",
      "Name: Price, dtype: int64\n",
      "2019    50\n",
      "2018    50\n",
      "2017    50\n",
      "2016    50\n",
      "2015    50\n",
      "2014    50\n",
      "2013    50\n",
      "2012    50\n",
      "2011    50\n",
      "2010    50\n",
      "2009    50\n",
      "Name: Year, dtype: int64\n",
      "Non Fiction    310\n",
      "Fiction        240\n",
      "Name: Genre, dtype: int64\n",
      "[None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "df_vals = [print(df[i].value_counts()) for i in df.columns]\n",
    "print(df_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing for Supervised Machine-Learning\n",
    "In the following markdown we see several features selected for mapping; those not selected have been noted as such. Judging from the select handful of features, we can see that the ideal target features would be the book ratings and the genre because of it's binary nature.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_mapping = lambda x: 1 if (x >= 4.7) else 0  #Scores 4.7 stars or higher = 1 \n",
    "df['Rating_map'] = df['User Rating'].map(rating_mapping)\n",
    "\n",
    "review_mapping = (lambda x: 1 if (x > 11953) else 0)  #Review count greater than mean = 1; not used as it reduced model accuracy\n",
    "df['Review_map'] = df['Reviews'].map(review_mapping)\n",
    "\n",
    "price_mapping = lambda x: 1 if (x > 13.1) else 0  #Price $13.10 or higher = 1; not used as it reduced model accuracy\n",
    "df['Price_map'] = df['Price'].map(price_mapping)\n",
    "\n",
    "genre_mapping = {\"Fiction\": 0, \"Non Fiction\": 1}\n",
    "df['Genre_map'] = df['Genre'].map(genre_mapping)\n",
    "\n",
    "df['Name Length'] = df['Name'].apply(lambda x: len(x))\n",
    "\n",
    "df['Name_word_count'] = df['Name'].apply(lambda x: len(re.findall(r' \\w+',x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating k-Nearest Neighbors model  \n",
    "Here we will create a subset of the original DataFrame of the mapped features we selected, removed any possible NaN values, and confirm all mapped outputs are in integer format. We will then instantiate the scaler and transform the subset of features for the KNN model. As noted before, the target features we deemed most appropriate have been saved as such. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating_map         False\n",
      "Genre_map          False\n",
      "Reviews            False\n",
      "Price              False\n",
      "Name_word_count    False\n",
      "dtype: bool\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avi_W\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "features = df[['Rating_map', 'Genre_map', 'Reviews', 'Price', 'Name_word_count']]\n",
    "subset_f = ['Rating_map', 'Genre_map', 'Reviews', 'Price', 'Name_word_count']\n",
    "features.dropna(subset=subset_f, inplace=True)\n",
    "\n",
    "keys = ['Rating_map', 'Genre_map', 'Reviews', 'Price', 'Name_word_count']\n",
    "values = []\n",
    "for i in keys:\n",
    "    values.append('int')\n",
    "kv = dict(zip(keys, values))\n",
    "features = features.astype(kv)\n",
    "\n",
    "print(features.isna().any()) #test for NA values#\n",
    "\n",
    "features_scaled = features[['Rating_map', 'Reviews', 'Price', 'Name_word_count']]\n",
    "features_scaled_2 = features[['Price', 'Reviews', 'Genre_map', 'Name_word_count']]\n",
    "\n",
    "x = features_scaled.values\n",
    "x2 = features_scaled_2.values\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()  #instantiate scaler\n",
    "min_max_scaler2 = preprocessing.MinMaxScaler() \n",
    "\n",
    "x_scaled = min_max_scaler.fit_transform(x)  #instance of scaler to transform data\n",
    "x_scaled_2 = min_max_scaler2.fit_transform(x2)\n",
    "\n",
    "features_scaled = pd.DataFrame(x_scaled, columns = features_scaled.columns)\n",
    "features_scaled_2 = pd.DataFrame(x_scaled_2, columns = features_scaled_2.columns)\n",
    "\n",
    "target = features['Genre_map']\n",
    "target2 = features['Rating_map']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data and training KNN model  \n",
    "In this case, we split the processed data into training and test sets, where the test size was set to a standard choice. Using the GridSearchCV function, we can obtain the most optimal K-neighbors; then use such parameters to tune the algorithm and provide a score. We can also use cross-validation to evaluate the model's score.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 22}\n",
      "{'n_neighbors': 18}\n",
      "[0.64545455 0.65454545 0.73636364 0.85454545 0.82727273]\n",
      "cv_scores mean: 0.7436363636363635\n",
      "[0.67567568 0.65765766 0.60909091 0.65137615 0.69724771]\n",
      "cv_scores mean: 0.6582096191270503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Avi_W\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2)\n",
    "x2_train, x2_test, y2_train, y2_test = train_test_split(features_scaled_2, target2, test_size=0.2)\n",
    "\n",
    "\n",
    "knn2 = KNeighborsClassifier()\n",
    "parameter_grid = {'n_neighbors': np.arange(1,25)}\n",
    "knn_gscv = GridSearchCV(knn2, parameter_grid, cv=5)\n",
    "knn_gscv.fit(features_scaled, target)  #change y\n",
    "print(knn_gscv.best_params_) #returns the most optimal neighbor parameter\n",
    "\n",
    "knn4 = KNeighborsClassifier()\n",
    "parameter_grid2 = {'n_neighbors': np.arange(1,25)}\n",
    "knn4_gscv = GridSearchCV(knn4, parameter_grid2, cv=5)\n",
    "knn4_gscv.fit(features_scaled_2, target2)  #change y\n",
    "print(knn4_gscv.best_params_) #returns the most optimal neighbor parameter\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 22) #n=22 Target 1\n",
    "knn.fit(x_train,y_train)\n",
    "cv_scores = cross_val_score(knn, features_scaled, target, cv=5)  #change y\n",
    "print(cv_scores)\n",
    "print('cv_scores mean: {}'.format(np.mean(cv_scores)))\n",
    "\n",
    "knn3 = KNeighborsClassifier(n_neighbors = 18) #n=18 Target 2\n",
    "knn3.fit(x2_train,y2_train)\n",
    "cv_scores2 = cross_val_score(knn3, features_scaled_2, target2, cv=5)  #change y\n",
    "print(cv_scores2)\n",
    "print('cv_scores mean: {}'.format(np.mean(cv_scores2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions with KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7090909090909091 0.7019489247311828 0.7041440217391304 0.7028031070584262\n",
      "0.6363636363636364 0.6363636363636364 0.6363636363636364 0.6363636363636364\n"
     ]
    }
   ],
   "source": [
    "predictions = knn.predict(x_test)\n",
    "print(accuracy_score(y_test, predictions), recall_score(y_test, predictions, average='macro'),\n",
    "      precision_score(y_test, predictions, average='macro'), f1_score(y_test, predictions, average='macro'))\n",
    "\n",
    "predictions2 = knn3.predict(x2_test)\n",
    "print(accuracy_score(y2_test, predictions2), recall_score(y2_test, predictions2, average='macro'),\n",
    "      precision_score(y2_test, predictions2, average='macro'), f1_score(y2_test, predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above predictions that when using the target feature 'Genre_map', on average, will receive better accuracy/scores around 70-79% as opposed to the target feature 'Rating_map'. Earlier when pre-processing, through trial and error, it can be shown that the prediction accuracy and scores are driven down when including these features in the model. That is of course when considering the various assumptions and choices in mapping. One potential reason why the genre map worked better is because the genre was already split between two options as opposed to the ratings. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
